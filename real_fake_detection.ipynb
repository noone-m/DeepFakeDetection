{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BQIFrsg7LLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd59392e-fdd8-4201-8388-aa1e53d64caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1HqVsYt6Ya0BRNC_mx0RCuPHQ9ELQi6GL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5IC-AA209xy",
        "outputId": "ae1c9076-2460-4364-d09c-da7841427d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1VUSrm3H1z4cCxSK-6kQCz7gcPDbiMn-x Copy of sd2.1.zip\n",
            "Processing file 1-8kL8LCEZ1PFUGnFmyvDpbZEogksHWDf Copy of simswap.zip\n",
            "Processing file 1J7eLYZV8Uxhv4RRrBAyz6y-vCX2a25n9 Copy of StyleGAN3.zip\n",
            "Processing file 1MItoK-E6xroejDNeRyfaB-3vl_9MjP04 Copy of VQGAN.zip\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VUSrm3H1z4cCxSK-6kQCz7gcPDbiMn-x\n",
            "From (redirected): https://drive.google.com/uc?id=1VUSrm3H1z4cCxSK-6kQCz7gcPDbiMn-x&confirm=t&uuid=48801f72-c648-4254-9fbb-4bf00928ca30\n",
            "To: /content/data/Copy of sd2.1.zip\n",
            "100% 6.21G/6.21G [00:51<00:00, 120MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-8kL8LCEZ1PFUGnFmyvDpbZEogksHWDf\n",
            "From (redirected): https://drive.google.com/uc?id=1-8kL8LCEZ1PFUGnFmyvDpbZEogksHWDf&confirm=t&uuid=9034bec9-671d-467c-9715-23cc5dffd373\n",
            "To: /content/data/Copy of simswap.zip\n",
            "100% 2.92G/2.92G [00:42<00:00, 68.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1J7eLYZV8Uxhv4RRrBAyz6y-vCX2a25n9\n",
            "From (redirected): https://drive.google.com/uc?id=1J7eLYZV8Uxhv4RRrBAyz6y-vCX2a25n9&confirm=t&uuid=3725a64e-455a-4daf-bdae-9e4bdf7ee2ec\n",
            "To: /content/data/Copy of StyleGAN3.zip\n",
            "100% 1.45G/1.45G [00:14<00:00, 103MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1MItoK-E6xroejDNeRyfaB-3vl_9MjP04\n",
            "From (redirected): https://drive.google.com/uc?id=1MItoK-E6xroejDNeRyfaB-3vl_9MjP04&confirm=t&uuid=7ae642bc-a27d-4d2f-839d-698a2efa72b4\n",
            "To: /content/data/Copy of VQGAN.zip\n",
            "100% 1.78G/1.78G [00:14<00:00, 123MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download real images"
      ],
      "metadata": {
        "id": "AMFoG62DD_RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1dHJdS0NZ6wpewbGA5B0PdIBS9gz28pdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52NNO9kxDKSi",
        "outputId": "e73530df-5130-42e9-d6fd-2d6dcc8d568e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dHJdS0NZ6wpewbGA5B0PdIBS9gz28pdb\n",
            "From (redirected): https://drive.google.com/uc?id=1dHJdS0NZ6wpewbGA5B0PdIBS9gz28pdb&confirm=t&uuid=36f1febd-e8ab-40f2-82f5-fdb975261b29\n",
            "To: /content/FaceForensics++_real_data_for_DF40.zip\n",
            "100% 2.97G/2.97G [00:47<00:00, 63.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMBC-fFk-kMv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir('data'):\n",
        "    if filename.endswith('.zip'):\n",
        "        zip_path = os.path.join('data', filename)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('data')\n",
        "        os.remove(zip_path)"
      ],
      "metadata": {
        "id": "XaSWO2PAuDXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('real_images')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "0rX-6KmAEZMc",
        "outputId": "71d76959-f5e6-4246-8e1d-62502e00e023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: 'real_images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2594090117.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'real_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'real_images'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = 'FaceForensics++_real_data_for_DF40.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "  zip_ref.extractall('real_images')\n",
        "os.remove(zip_path)"
      ],
      "metadata": {
        "id": "ut7-DjCMEqzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = os.walk('data')\n",
        "for i in range(10):\n",
        "  print(next(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7IzvSHCHChp",
        "outputId": "947f979e-d4a4-4d56-e7fa-64a346766af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('data', ['VQGAN', 'real_images', 'StyleGAN3', 'sd2.1', 'simswap', 'fake_images'], [])\n",
            "('data/VQGAN', ['260', '447', '595', '180', '353', '939', '365', '216', '923', '212', '196', '904', '690', '751', '020', '684', '858', '692', '427', '770', '950', '912', '269', '315', '486', '460', '925', '602', '872', '934', '335', '205', '451', '294', '210', '819', '643', '800', '615', '172', '964', '954', '471', '075', '405', '778', '883', '888', '914', '087', '963', '978', '969', '681', '826', '258', '417', '283', '922', '532', '209', '947', '893', '756', '243', '152', '628', '907', '641', '121', '304', '652', '221', '499', '723', '131', '621', '066', '298', '823', '440', '873', '407', '640', '537', '256', '992', '046', '828', '126', '931', '282', '649', '134', '520', '563', '666', '867', '559', '198', '410', '688', '887', '370', '444', '674', '300', '902', '295', '604', '487', '678', '574', '777', '565', '629', '324', '021', '382', '843', '892', '346', '215', '693', '593', '557', '390', '566', '663', '340', '483', '081', '694', '951', '275', '040', '156', '994', '673', '536', '900', '160', '882', '712', '761', '111', '277', '322', '203', '031', '272', '270', '913', '245', '337', '174', '377', '567', '782', '519', '877', '967', '043', '528', '166', '098', '764', '535', '213', '242', '392', '195', '827', '758', '362', '409', '361', '470', '077', '966', '424', '266', '079', '063', '869', '084', '861', '030', '247', '022', '838', '348', '804', '137', '286', '356', '813', '307', '746', '644', '658', '211', '810', '273', '790', '475', '783', '589', '117', '886', '795', '302', '416', '655', '866', '338', '246', '442', '627', '241', '408', '599', '679', '657', '025', '569', '637', '187', '948', '013', '279', '936', '092', '082', '393', '202', '845', '561', '708', '889', '489', '039', '659', '797', '768', '997', '368', '671', '610', '132', '431', '175', '223', '218', '984', '109', '545', '702', '580', '053', '453', '127', '935', '817', '400', '168', '820', '551', '667', '027', '713', '597', '052', '930', '251', '289', '157', '901', '715', '193', '070', '145', '287', '197', '449', '645', '120', '518', '736', '965', '194', '622', '740', '478', '836', '229', '080', '250', '802', '999', '668', '248', '689', '616', '624', '875', '042', '411', '463', '784', '979', '647', '231', '796', '008', '095', '856', '019', '794', '878', '538', '139', '103', '829', '391', '339', '687', '776', '766', '642', '591', '284', '575', '957', '500', '105', '841', '619', '586', '686', '045', '848', '760', '700', '464', '331', '378', '895', '341', '476', '677', '490', '941', '831', '491', '747', '757', '909', '876', '004', '349', '271', '833', '336', '805', '835', '818', '153', '104', '072', '253', '554', '938', '748', '071', '222', '508', '159', '799', '793', '809', '234', '564', '276', '240', '101', '850', '268', '274', '032', '074', '961', '779', '560', '881', '330', '437', '952', '412', '648', '738', '767', '834', '107', '501', '259', '903', '119', '016', '188', '504', '017', '493', '167', '832', '771', '998', '958', '136', '397', '859', '226', '228', '959', '006', '497', '323', '354', '730', '709', '993', '697', '438', '765', '871', '603', '484', '542', '840', '825', '402', '010', '406', '465', '217', '116', '076', '749', '530', '150', '108', '359', '426', '199', '446', '267', '502', '472', '587', '996', '477', '244', '982', '516', '364', '956', '750', '360', '533', '940', '097', '309', '985', '534', '960', '568', '990', '350', '091', '110', '792', '261', '192', '646', '553', '334', '572', '481', '526', '870', '325', '366', '592', '676', '544', '333', '558', '734', '605', '762', '143', '413', '062', '632', '089', '976', '769', '415', '791', '106', '844', '722', '292', '612', '394', '672', '811', '083', '007', '232', '055', '041', '088', '293', '357', '396', '200', '096', '664', '363', '849', '556', '620', '894', '419', '090', '054', '618', '441', '383', '733', '594', '028', '038', '656', '874', '224', '018', '991', '332', '506', '971', '555', '100', '312', '754', '573', '680', '803', '816', '482', '613', '086', '398', '699', '983', '301', '414', '207', '009', '162', '753', '588', '598', '164', '703', '735', '496', '905', '065', '133', '458', '448', '860', '505', '401', '596', '351', '962', '118', '254', '238', '181', '456', '824', '355', '151', '946', '395', '885', '474', '785', '590', '498', '879', '511', '430', '204', '473', '814', '129', '303', '173', '049', '531', '061', '225', '510', '704', '884', '807', '235', '968', '685', '436', '144', '977', '443', '763', '933', '539', '385', '037', '099', '798', '543', '064', '001', '130', '236', '439', '908', '455', '189', '305', '846', '313', '857', '428', '469', '626', '915', '651', '635', '815', '581', '014', '611', '562', '503', '023', '011', '614', '369', '737', '285', '191', '165', '752', '509', '155', '822', '617', '937', '085', '163'], [])\n",
            "('data/VQGAN/260', [], ['003886.png', '002639.png', '017705.png', '020618.png', '006639.png', '014584.png', '007197.png', '011836.png', '007240.png', '010858.png', '000388.png', '012056.png', '000847.png', '018380.png', '020559.png', '017881.png', '018056.png', '019679.png', '010991.png', '012961.png', '011548.png', '009220.png', '007890.png', '013369.png', '000639.png', '000980.png', '009882.png', '016007.png', '017173.png', '002116.png', '020869.png', '008133.png'])\n",
            "('data/VQGAN/447', [], ['010912.png', '015494.png', '018489.png', '017104.png', '020591.png', '007289.png', '014283.png', '004708.png', '012014.png', '020823.png', '009221.png', '017059.png', '008874.png', '013730.png', '007760.png', '020923.png', '011422.png', '016280.png', '007113.png', '002288.png', '014842.png', '016918.png', '011691.png', '010812.png', '017704.png', '016643.png', '012299.png', '007834.png', '013427.png', '006184.png', '020499.png', '014561.png'])\n",
            "('data/VQGAN/595', [], ['018962.png', '011677.png', '007853.png', '013569.png', '014527.png', '012657.png', '010968.png', '001595.png', '015631.png', '019378.png', '015488.png', '001218.png', '018970.png', '011592.png', '013732.png', '003668.png', '008385.png', '011397.png', '008574.png', '013525.png', '000258.png', '010416.png', '020427.png', '015906.png', '012026.png', '011394.png', '019735.png', '004460.png', '007714.png', '000040.png', '002569.png', '018080.png'])\n",
            "('data/VQGAN/180', [], ['013305.png', '004530.png', '000596.png', '012187.png', '018923.png', '015512.png', '014601.png', '017056.png', '007469.png', '016512.png', '020688.png', '011014.png', '019712.png', '015049.png', '013502.png', '011032.png', '008822.png', '013629.png', '013918.png', '004349.png', '017268.png', '011854.png', '013862.png', '009195.png', '011853.png', '010785.png', '001003.png', '006342.png', '013225.png', '002724.png', '009502.png', '011525.png'])\n",
            "('data/VQGAN/353', [], ['007110.png', '019087.png', '005670.png', '012584.png', '002264.png', '008215.png', '013406.png', '003806.png', '011135.png', '008584.png', '017951.png', '007440.png', '020787.png', '005211.png', '013559.png', '019468.png', '012906.png', '003694.png', '003294.png', '001850.png', '014338.png', '019235.png', '004596.png', '004225.png', '015838.png', '014756.png', '005376.png', '003501.png', '017179.png', '005263.png', '016617.png', '005250.png'])\n",
            "('data/VQGAN/939', [], ['002041.png', '006196.png', '008634.png', '002078.png', '014588.png', '009609.png', '017596.png', '010875.png', '007880.png', '017891.png', '005502.png', '003452.png', '000409.png', '007493.png', '017054.png', '019670.png', '018806.png', '002867.png', '007581.png', '019242.png', '016965.png', '000440.png', '004216.png', '006476.png', '014207.png', '019137.png', '004374.png', '017414.png', '017264.png', '001522.png', '007403.png', '003262.png'])\n",
            "('data/VQGAN/365', [], ['018793.png', '012804.png', '013816.png', '019322.png', '004254.png', '013827.png', '006167.png', '004643.png', '017030.png', '012703.png', '004303.png', '006293.png', '006499.png', '000181.png', '004923.png', '015568.png', '002159.png', '008455.png', '005920.png', '002813.png', '010830.png', '015301.png', '001692.png', '010448.png', '013613.png', '011637.png', '003870.png', '015708.png', '014654.png', '011393.png', '014386.png', '018969.png'])\n",
            "('data/VQGAN/216', [], ['014095.png', '006220.png', '017508.png', '007520.png', '014468.png', '017372.png', '004435.png', '017820.png', '010685.png', '003406.png', '009075.png', '004312.png', '011225.png', '018703.png', '002828.png', '013634.png', '012326.png', '007055.png', '011888.png', '003716.png', '019466.png', '003299.png', '012349.png', '004382.png', '015938.png', '003155.png', '009999.png', '009441.png', '019479.png', '013223.png', '014754.png', '014827.png'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "os.makedirs('data/fake_images', exist_ok=True)\n",
        "\n",
        "for root, dirs, files in os.walk('data'):\n",
        "  if root != 'data/fake_images':  # Don't try to move files from the destination\n",
        "    for file in files:\n",
        "      # Assuming all image files have common extensions like .jpg, .png, etc.\n",
        "      if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
        "        src_path = os.path.join(root, file)\n",
        "        dst_path = os.path.join('data/fake_images', file)\n",
        "        # Add a check to prevent moving files to themselves in case of subdirectories\n",
        "        if src_path != dst_path:\n",
        "          shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "jZb6YGcMGIJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"there is {len(os.listdir('data/fake_images'))} fake images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11o5TXMH-_o",
        "outputId": "37d6298f-bc13-4a71-a985-4683518aa509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is 88430 fake images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2 = os.walk('real_images')\n",
        "for i in range(10):\n",
        "  print(next(w2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC15uD-yJjy3",
        "outputId": "d59d683f-559e-44ad-efcb-0c913bf05e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('real_images', ['FaceForensics++'], [])\n",
            "('real_images/FaceForensics++', ['original_sequences'], [])\n",
            "('real_images/FaceForensics++/original_sequences', ['youtube'], [])\n",
            "('real_images/FaceForensics++/original_sequences/youtube', ['c23'], [])\n",
            "('real_images/FaceForensics++/original_sequences/youtube/c23', ['frames'], [])\n",
            "('real_images/FaceForensics++/original_sequences/youtube/c23/frames', ['260', '633', '585', '447', '595', '180', '353', '939', '365', '216', '654', '923', '212', '583', '196', '342', '904', '690', '751', '639', '020', '684', '858', '035', '692', '427', '770', '950', '912', '249', '269', '058', '230', '315', '524', '486', '780', '696', '384', '316', '460', '925', '602', '872', '934', '335', '205', '691', '812', '451', '944', '949', '855', '294', '210', '432', '819', '643', '800', '615', '172', '964', '525', '051', '577', '381', '190', '954', '471', '291', '075', '405', '778', '883', '888', '914', '087', '529', '963', '970', '978', '969', '681', '826', '258', '417', '283', '922', '182', '532', '209', '947', '893', '050', '386', '756', '243', '152', '628', '907', '641', '121', '304', '652', '221', '499', '015', '723', '219', '131', '621', '932', '066', '298', '716', '607', '823', '440', '873', '407', '640', '579', '537', '256', '992', '171', '046', '296', '828', '044', '126', '931', '282', '701', '649', '134', '520', '067', '563', '921', '404', '666', '867', '559', '198', '320', '410', '029', '688', '887', '370', '731', '444', '308', '541', '674', '300', '902', '295', '604', '450', '078', '487', '678', '233', '574', '521', '777', '565', '547', '629', '324', '380', '021', '382', '843', '892', '169', '346', '215', '693', '682', '593', '557', '390', '566', '663', '340', '445', '483', '081', '694', '358', '951', '275', '239', '728', '040', '156', '994', '673', '743', '536', '900', '459', '280', '160', '882', '712', '955', '761', '111', '576', '255', '277', '148', '322', '203', '031', '695', '272', '142', '270', '913', '245', '337', '174', '377', '567', '782', '519', '122', '278', '877', '967', '043', '034', '910', '528', '166', '098', '764', '919', '048', '535', '213', '073', '242', '392', '125', '195', '827', '758', '917', '362', '409', '361', '470', '077', '966', '424', '266', '488', '079', '063', '670', '869', '084', '861', '030', '247', '022', '838', '788', '920', '348', '804', '137', '286', '710', '665', '717', '356', '813', '307', '746', '644', '658', '211', '810', '273', '790', '669', '842', '475', '783', '328', '589', '707', '429', '117', '886', '795', '302', '227', '416', '655', '141', '852', '479', '866', '338', '246', '442', '627', '929', '068', '241', '408', '495', '123', '599', '679', '657', '897', '025', '569', '552', '637', '187', '306', '948', '013', '279', '675', '936', '092', '082', '393', '202', '718', '845', '561', '837', '372', '708', '889', '489', '039', '264', '634', '057', '257', '659', '650', '705', '945', '252', '797', '768', '772', '997', '953', '265', '726', '368', '158', '671', '610', '517', '132', '431', '175', '223', '170', '891', '218', '149', '984', '109', '545', '702', '580', '053', '453', '127', '466', '935', '817', '400', '168', '507', '880', '820', '551', '667', '027', '713', '597', '052', '930', '036', '251', '289', '157', '901', '715', '193', '899', '070', '660', '454', '145', '287', '197', '461', '449', '645', '120', '518', '973', '736', '965', '194', '622', '214', '864', '740', '478', '847', '836', '229', '080', '250', '802', '540', '999', '668', '347', '989', '248', '896', '311', '689', '732', '310', '616', '624', '875', '042', '411', '773', '463', '161', '784', '979', '435', '647', '231', '796', '008', '095', '711', '856', '019', '794', '878', '317', '113', '185', '124', '418', '974', '630', '538', '114', '550', '462', '625', '139', '206', '103', '829', '391', '339', '687', '776', '220', '766', '642', '112', '591', '284', '575', '957', '389', '183', '452', '500', '105', '841', '327', '619', '586', '512', '686', '801', '045', '848', '760', '102', '047', '700', '059', '464', '839', '331', '378', '895', '341', '476', '677', '056', '490', '941', '831', '491', '135', '747', '757', '909', '024', '876', '004', '606', '349', '271', '480', '833', '336', '570', '865', '805', '835', '609', '138', '434', '818', '153', '379', '104', '608', '186', '072', '253', '321', '554', '938', '748', '071', '987', '494', '222', '508', '159', '799', '003', '793', '809', '943', '234', '564', '276', '240', '101', '850', '387', '268', '274', '032', '074', '961', '779', '560', '881', '330', '661', '437', '928', '952', '412', '648', '738', '767', '834', '107', '501', '259', '903', '119', '016', '942', '188', '504', '017', '493', '911', '167', '832', '771', '998', '343', '958', '136', '851', '397', '859', '653', '226', '739', '228', '959', '006', '497', '323', '523', '354', '730', '399', '709', '986', '571', '662', '371', '993', '697', '201', '924', '438', '765', '388', '871', '603', '484', '060', '542', '840', '724', '546', '988', '825', '128', '176', '402', '010', '406', '465', '217', '116', '076', '749', '755', '775', '530', '906', '853', '150', '485', '108', '359', '426', '319', '199', '446', '344', '631', '352', '267', '502', '288', '472', '587', '996', '477', '244', '982', '516', '115', '787', '364', '237', '956', '515', '750', '360', '533', '940', '492', '863', '097', '376', '309', '985', '534', '960', '568', '990', '350', '091', '110', '179', '792', '261', '980', '012', '192', '646', '000', '916', '553', '719', '334', '572', '262', '926', '481', '526', '870', '325', '366', '592', '676', '544', '333', '177', '636', '558', '734', '605', '314', '762', '600', '143', '413', '062', '154', '632', '089', '976', '769', '373', '415', '791', '106', '706', '844', '722', '292', '638', '698', '612', '394', '672', '721', '184', '811', '083', '007', '433', '232', '774', '033', '055', '789', '041', '088', '293', '423', '357', '396', '200', '096', '664', '363', '849', '556', '620', '894', '419', '090', '054', '618', '578', '441', '178', '383', '733', '468', '594', '028', '038', '069', '522', '656', '874', '224', '018', '549', '759', '991', '332', '506', '971', '555', '100', '312', '754', '573', '680', '803', '816', '482', '927', '613', '745', '086', '398', '972', '422', '699', '781', '983', '301', '414', '207', '009', '162', '753', '588', '598', '164', '703', '854', '208', '094', '735', '496', '329', '905', '065', '133', '458', '448', '860', '297', '505', '714', '401', '146', '601', '596', '351', '962', '725', '326', '623', '118', '254', '238', '181', '456', '824', '727', '584', '355', '457', '151', '946', '147', '093', '395', '862', '885', '786', '474', '720', '785', '821', '742', '263', '590', '498', '879', '511', '375', '002', '430', '204', '473', '467', '814', '299', '514', '403', '129', '303', '173', '049', '531', '918', '061', '225', '683', '425', '830', '510', '704', '890', '981', '995', '884', '527', '807', '235', '968', '808', '685', '898', '436', '144', '977', '443', '763', '933', '729', '539', '420', '385', '868', '037', '099', '798', '543', '064', '367', '001', '290', '421', '130', '236', '741', '975', '439', '806', '908', '455', '189', '005', '305', '846', '313', '857', '428', '469', '548', '626', '345', '915', '651', '635', '318', '815', '581', '014', '611', '562', '503', '023', '744', '026', '140', '011', '374', '614', '369', '737', '285', '191', '165', '752', '509', '155', '822', '513', '617', '937', '582', '085', '163'], [])\n",
            "('real_images/FaceForensics++/original_sequences/youtube/c23/frames/260', [], ['029.png', '281.png', '301.png', '223.png', '165.png', '174.png', '087.png', '194.png', '135.png', '126.png', '009.png', '038.png', '233.png', '145.png', '213.png', '097.png', '116.png', '058.png', '000.png', '291.png', '203.png', '106.png', '019.png', '271.png', '077.png', '242.png', '155.png', '262.png', '184.png', '252.png', '048.png', '067.png'])\n",
            "('real_images/FaceForensics++/original_sequences/youtube/c23/frames/633', [], ['428.png', '263.png', '461.png', '181.png', '395.png', '478.png', '313.png', '115.png', '494.png', '445.png', '082.png', '247.png', '362.png', '214.png', '016.png', '329.png', '412.png', '148.png', '346.png', '049.png', '000.png', '098.png', '296.png', '065.png', '280.png', '164.png', '197.png', '131.png', '379.png', '032.png', '511.png', '230.png'])\n",
            "('real_images/FaceForensics++/original_sequences/youtube/c23/frames/585', [], ['034.png', '334.png', '092.png', '115.png', '069.png', '046.png', '173.png', '311.png', '300.png', '011.png', '196.png', '346.png', '138.png', '323.png', '207.png', '000.png', '023.png', '358.png', '057.png', '127.png', '254.png', '288.png', '103.png', '080.png', '242.png', '161.png', '277.png', '184.png', '219.png', '150.png', '265.png', '230.png'])\n",
            "('real_images/FaceForensics++/original_sequences/youtube/c23/frames/447', [], ['062.png', '020.png', '115.png', '136.png', '125.png', '094.png', '188.png', '283.png', '083.png', '104.png', '178.png', '052.png', '220.png', '293.png', '304.png', '325.png', '000.png', '272.png', '209.png', '010.png', '041.png', '199.png', '146.png', '031.png', '241.png', '167.png', '157.png', '314.png', '073.png', '251.png', '262.png', '230.png'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "os.makedirs('data/real_images', exist_ok=True)\n",
        "\n",
        "for root, dirs, files in os.walk('real_images'):\n",
        "  for file in files:\n",
        "    # Assuming all image files have common extensions like .jpg, .png, etc.\n",
        "    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
        "      src_path = os.path.join(root, file)\n",
        "\n",
        "      # Create a unique name using parent folder structure so images with same name don't override each others\n",
        "      relative_path = os.path.relpath(src_path, 'real_images')\n",
        "      folder_prefix = os.path.dirname(relative_path).replace(os.sep, '_')  # e.g., FaceForensics++_original_sequences_youtube_c23_frames_315\n",
        "      unique_name = f\"{folder_prefix}_{file}\"  # e.g., 315_000.png → FaceForensics++_..._315_000.png\n",
        "      dst_path = os.path.join('data/real_images', unique_name)\n",
        "      shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "rKiNGtynJNIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"there is {len(os.listdir('data/real_images'))} real images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLnNStN9J-Ij",
        "outputId": "d86f09de-f768-4243-fc43-f3b250f66be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is 31949 real images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**there is data imbalance fake images are more than real ones**\n",
        "- we are going to solve this imbalnce using weighted loss to penalize more on real images"
      ],
      "metadata": {
        "id": "bwGQkfcLQql5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "# class DeepFakeDataset(Dataset):\n",
        "#     def __init__(self, data_dir, transform=None):\n",
        "#         self.data_dir = data_dir\n",
        "#         self.transform = transform\n",
        "#         self.fake_images = [os.path.join(data_dir, 'fake_images', img) for img in os.listdir(os.path.join(data_dir, 'fake_images'))]\n",
        "#         self.real_images = [os.path.join(data_dir, 'real_images', img) for img in os.listdir(os.path.join(data_dir, 'real_images'))]\n",
        "#         self.all_images = self.fake_images + self.real_images\n",
        "#         self.labels = [1] * len(self.fake_images) + [0] * len(self.real_images) # 1 for fake, 0 for real\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.all_images)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.all_images[idx]\n",
        "#         label = self.labels[idx]\n",
        "\n",
        "#         image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return image, label\n",
        "\n",
        "# # Example usage:\n",
        "# # Define transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)), # Resize images to a consistent size\n",
        "#     transforms.ToTensor(),         # Convert PIL image to PyTorch tensor\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize\n",
        "# ])\n",
        "\n",
        "# # Create the dataset\n",
        "# dataset = DeepFakeDataset(data_dir='data', transform=transform)\n",
        "\n",
        "# # Create a DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# # Example of iterating through the DataLoader\n",
        "# # for images, labels in dataloader:\n",
        "# #     print(\"Batch of images shape:\", images.shape)\n",
        "# #     print(\"Batch of labels shape:\", labels.shape)\n",
        "# #     break # Just show one batch\n"
      ],
      "metadata": {
        "id": "QphuarHIRWNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0ry9L6mIvHL",
        "outputId": "e5ae6f1b-a8cd-40e7-887d-92c58a9ef57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.11/dist-packages (0.7.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from efficientnet_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->efficientnet_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guZxukG8PoJx"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from transformers import CLIPModel, CLIPProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr1ZPObYQJom",
        "outputId": "37d0c7b6-1948-4697-e4a7-85796149187e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "#load models\n",
        "efficientnet_model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1BlsHXSj4A",
        "outputId": "bd141260-fc0e-452f-8b98-6ba8309250ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3-5): 3 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (7-9): 3 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (10): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11-15): 5 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (16): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (17-21): 5 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (22): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (23-29): 7 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (30): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (31): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.4, inplace=False)\n",
              "  (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# put the models in evaluation state to stop dropout and use (running statistics instead of batch statistics)\n",
        "clip_model.eval()\n",
        "efficientnet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o91zXWWhWm9z",
        "outputId": "de1315a5-62de-4b7f-ce71-b2b9982cb23a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvsSWdvJWulX"
      },
      "outputs": [],
      "source": [
        "efficientnet_model = efficientnet_model.to(device)\n",
        "clip_model = clip_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMH439DqPNB7"
      },
      "outputs": [],
      "source": [
        "# efficientNet Preprocessing Pipeline\n",
        "efficientNet_preprocess_transforms = transforms.Compose([\n",
        "    transforms.Resize((380, 380)),  # Resize to 380x380\n",
        "    transforms.ToTensor() ,  # Convert to tensor and scale to [0, 1]\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
        "])\n",
        "def efficientNet_preprocess(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    efficientNet_image = efficientNet_preprocess_transforms(image)\n",
        "    efficientNet_image = efficientNet_image.unsqueeze(0)\n",
        "    efficientNet_image = efficientNet_image.to(device, dtype=torch.float32)\n",
        "    return efficientNet_image\n",
        "\n",
        "# Forward pass (example)\n",
        "with torch.no_grad():\n",
        "    output = efficientnet_model(efficientNet_preprocess(\"data/fake_images/seed4109.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jlGoQs6X3Ra",
        "outputId": "00884505-808a-4562-e29f-195b1fbe2d32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j3pMqBMnY-gc",
        "outputId": "1f248b1e-5f0b-41b4-96ac-f010c9eba2fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3-5): 3 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (7-9): 3 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (10): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11-15): 5 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (16): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (17-21): 5 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (22): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (23-29): 7 x MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (30): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (31): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.4, inplace=False)\n",
              "  (_fc): Linear(in_features=1792, out_features=1000, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "efficientnet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwcVTt_hYqeo",
        "outputId": "1c9ff203-3282-4a1b-e5fb-48933cf03f27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1792"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "efficientnet_model._fc.in_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RKnBZ2RcYpn",
        "outputId": "bc1ce966-cfbf-4d11-bd7c-56a818af2e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 768])\n"
          ]
        }
      ],
      "source": [
        "# CLIP Preprocessing pipline\n",
        "\n",
        "def clip_preprocess(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    clip_image = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    # Move inputs to the device\n",
        "    clip_image = {k: v.to(device) for k, v in clip_image.items()}\n",
        "    return clip_image\n",
        "\n",
        "# Forward pass with CLIP model\n",
        "with torch.no_grad():\n",
        "    clip_outputs = clip_model.get_image_features(**clip_preprocess(\"data/fake_images/seed4109.png\"))\n",
        "# clip_outputs now contains the image features from the CLIP model\n",
        "print(clip_outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USG25Z3Zgfqt",
        "outputId": "0158efde-295a-4ad6-c3d7-08ed6e15e10e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "clip_model.visual_projection.out_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.block(x) + x)"
      ],
      "metadata": {
        "id": "0RVTnq_2wbFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI7ZQImM_Bzw",
        "outputId": "af3fdf48-c3d4-4f37-e845-4ebab542d0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b4\n"
          ]
        }
      ],
      "source": [
        "# To use the model, you'll need to ensure your image preprocessing matches the input requirements of both models.\n",
        "# A common approach is to resize the image to both sizes and preprocess accordingly, then pass the appropriately\n",
        "# preprocessed tensors to the forward method.\n",
        "\n",
        "# For instance, if your dataset provides images in a standard format:\n",
        "# 1. Load image\n",
        "# 2. Preprocess for EfficientNet (resize to 380x380, normalize etc.) -> effnet_input\n",
        "# 3. Preprocess for CLIP (resize to 224x224, normalize with CLIP's method) -> clip_input\n",
        "# 4. Call model: output = model(effnet_input, clip_input)\n",
        "\n",
        "# note ->:  the current forward method expects a single `images` input. You might need to modify the forward\n",
        "# method to accept two different inputs or handle the preprocessing within the dataset/dataloader.\n",
        "\n",
        "# Assuming for demonstration purposes that a single input `images` contains both appropriately sized tensors.\n",
        "# In a real scenario, you'd need a more sophisticated data loading and preprocessing pipeline.\n",
        "\n",
        "# Let's redefine the forward method to take two distinct inputs for clarity in a real scenario.\n",
        "class HybridModelImplemtation(nn.Module):\n",
        "    def __init__(self, efficientnet_model, clip_model, num_classes=2):\n",
        "        super(HybridModelImplemtation, self).__init__()\n",
        "        self.efficientnet = efficientnet_model\n",
        "        self.clip_model = clip_model\n",
        "\n",
        "        for param in self.efficientnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        eff_out = self.efficientnet._fc.in_features\n",
        "        clip_out = self.clip_model.visual_projection.out_features\n",
        "        self.efficientnet._fc = nn.Identity()\n",
        "\n",
        "        combined_dim = eff_out + clip_out\n",
        "\n",
        "        self.fc_proj = nn.Linear(combined_dim, 512)\n",
        "        self.res_block1 = ResidualBlock(512)\n",
        "        self.res_block2 = ResidualBlock(512)\n",
        "        self.res_block3 = ResidualBlock(512)\n",
        "        self.res_block4 = ResidualBlock(512)\n",
        "        self.res_block5 = ResidualBlock(512)\n",
        "        self.res_block6 = ResidualBlock(512)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.output_layer = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, efficientnet_image, clip_image):\n",
        "        with torch.no_grad():\n",
        "            efficientnet_features = self.efficientnet(efficientnet_image)\n",
        "        with torch.no_grad():\n",
        "            clip_features = self.clip_model.get_image_features(**clip_image)\n",
        "\n",
        "        x = torch.cat((efficientnet_features, clip_features), dim=1)\n",
        "        x = self.fc_proj(x)\n",
        "        x = self.res_block1(x)\n",
        "        x = self.res_block2(x)\n",
        "        x = self.res_block3(x)\n",
        "        x = self.res_block4(x)\n",
        "        x = self.res_block5(x)\n",
        "        x = self.res_block6(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "efficientnet_model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "model = HybridModelImplemtation(efficientnet_model=efficientnet_model, clip_model=clip_model,num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mcoQS8vsfj0",
        "outputId": "0545ccd7-5b77-4b1e-89ca-c3c6d5e0d468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[-0.2576, -0.0016]], device='cuda:0')\n",
            "Predicted probabilities: tensor([[0.4363, 0.5637]], device='cuda:0')\n",
            "Predicted class index: 1\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the image\n",
        "image_path = \"data/fake_images/seed4109.png\"\n",
        "efficientnet_image = efficientNet_preprocess(image_path)\n",
        "clip_image = clip_preprocess(image_path)\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Perform inference\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    output = model(efficientnet_image, clip_image)\n",
        "\n",
        "print(\"Model output:\", output)\n",
        "\n",
        "# Example of getting predicted class (assuming binary classification and output are logits)\n",
        "probabilities = torch.softmax(output, dim=1)\n",
        "predicted_class = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "print(\"Predicted probabilities:\", probabilities)\n",
        "print(\"Predicted class index:\", predicted_class.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nfQsLdodbGW"
      },
      "outputs": [],
      "source": [
        "# Now you would proceed with defining your loss function, optimizer, dataset, dataloader, and training loop.\n",
        "# You'll need to create a custom Dataset and Dataloader that handles loading your image data and applying the\n",
        "# correct preprocessing steps for both EfficientNet and CLIP."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DeepFakeDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        efficientnet_image = efficientNet_preprocess(img_path)\n",
        "        clip_image = clip_preprocess(img_path)\n",
        "\n",
        "        # Move inputs to the device within the dataset item\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        efficientnet_image = efficientnet_image.squeeze(0).to(device, dtype=torch.float32)\n",
        "        clip_image = {k: v.squeeze(0).to(device) for k, v in clip_image.items()} # Squeeze batch dim added by processor\n",
        "\n",
        "        return efficientnet_image, clip_image, label"
      ],
      "metadata": {
        "id": "c_RJTKm7WHNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lySFaiWki-hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for the Dataset\n",
        "data_dir = 'data'\n",
        "fake_image_paths = [os.path.join(data_dir, 'fake_images', img) for img in os.listdir(os.path.join(data_dir, 'fake_images'))]\n",
        "real_image_paths = [os.path.join(data_dir, 'real_images', img) for img in os.listdir(os.path.join(data_dir, 'real_images'))]\n",
        "\n",
        "all_image_paths = fake_image_paths + real_image_paths\n",
        "all_labels = [1] * len(fake_image_paths) + [0] * len(real_image_paths) # 1 for fake, 0 for real\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    all_image_paths, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create the training and validation datasets\n",
        "train_dataset = DeepFakeDataset(\n",
        "    image_paths=train_paths,\n",
        "    labels=train_labels,\n",
        ")\n",
        "\n",
        "val_dataset = DeepFakeDataset(\n",
        "    image_paths=val_paths,\n",
        "    labels=val_labels,\n",
        ")"
      ],
      "metadata": {
        "id": "EWUcGUybZHdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'there is {len(train_dataset)} training image')\n",
        "print(f'there is {len(val_dataset)} val image')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz81fc7Hb04p",
        "outputId": "ab16eb4e-7862-417d-ab35-b5e4dca38da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there is 96303 training image\n",
            "there is 24076 val image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.unique(all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXM8UI4gdDtO",
        "outputId": "951c55c4-d134-48ed-cd4b-3e663bc5560e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Loss Function (with weighting for imbalance)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(all_labels),\n",
        "    y=all_labels\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "class_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMcUmyBBcvwr",
        "outputId": "1d6b67da-d007-4ed1-df41-65320a07e5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.8839, 0.6806], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "FC0kCPmedLAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = float('inf')"
      ],
      "metadata": {
        "id": "KRGdL-ZXsx97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    batch_loss = 0.0\n",
        "\n",
        "    for i, (efficientnet_images, clip_images, labels) in enumerate(train_dataloader):\n",
        "        labels = labels.to(device)\n",
        "        efficientnet_images = efficientnet_images.to(device)\n",
        "        clip_images = {k: v.to(device) for k, v in clip_images.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(efficientnet_images, clip_images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * efficientnet_images.size(0)\n",
        "        batch_loss += loss.item()\n",
        "\n",
        "        # Print loss every 64 batches\n",
        "        if (i + 1) % 64 == 0:\n",
        "            avg_batch_loss = batch_loss / 64\n",
        "            print(f\"Epoch {epoch+1}, Batch {i+1}, Avg Loss over last 64 batches: {avg_batch_loss:.4f}\")\n",
        "            batch_loss = 0.0  # Reset batch_loss\n",
        "\n",
        "            if avg_batch_loss < best_loss:\n",
        "                best_loss = avg_batch_loss\n",
        "                torch.save(model.state_dict(), 'best_model.pth')\n",
        "                print(f\"Model saved with loss {best_loss:.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GnKvyLe7ZC7w",
        "outputId": "676babcc-4f28-4987-8038-f9e5dd6ab05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 64, Avg Loss over last 64 batches: 0.4289\n",
            "Model saved with loss 0.4289\n",
            "Epoch 1, Batch 128, Avg Loss over last 64 batches: 0.4692\n",
            "Epoch 1, Batch 192, Avg Loss over last 64 batches: 0.4056\n",
            "Model saved with loss 0.4056\n",
            "Epoch 1, Batch 256, Avg Loss over last 64 batches: 0.4274\n",
            "Epoch 1, Batch 320, Avg Loss over last 64 batches: 0.4213\n",
            "Epoch 1, Batch 384, Avg Loss over last 64 batches: 0.4639\n",
            "Epoch 1, Batch 448, Avg Loss over last 64 batches: 0.4233\n",
            "Epoch 1, Batch 512, Avg Loss over last 64 batches: 0.4091\n",
            "Epoch 1, Batch 576, Avg Loss over last 64 batches: 0.4348\n",
            "Epoch 1, Batch 640, Avg Loss over last 64 batches: 0.4205\n",
            "Epoch 1, Batch 704, Avg Loss over last 64 batches: 0.4748\n",
            "Epoch 1, Batch 768, Avg Loss over last 64 batches: 0.4542\n",
            "Epoch 1, Batch 832, Avg Loss over last 64 batches: 0.4487\n",
            "Epoch 1, Batch 896, Avg Loss over last 64 batches: 0.5301\n",
            "Epoch 1, Batch 960, Avg Loss over last 64 batches: 0.4845\n",
            "Epoch 1, Batch 1024, Avg Loss over last 64 batches: 0.5338\n",
            "Epoch 1, Batch 1088, Avg Loss over last 64 batches: 0.4590\n",
            "Epoch 1, Batch 1152, Avg Loss over last 64 batches: 0.4636\n",
            "Epoch 1, Batch 1216, Avg Loss over last 64 batches: 0.4831\n",
            "Epoch 1, Batch 1280, Avg Loss over last 64 batches: 0.4747\n",
            "Epoch 1, Batch 1344, Avg Loss over last 64 batches: 0.4408\n",
            "Epoch 1, Batch 1408, Avg Loss over last 64 batches: 0.4250\n",
            "Epoch 1, Batch 1472, Avg Loss over last 64 batches: 0.4383\n",
            "Epoch 1, Batch 1536, Avg Loss over last 64 batches: 0.4190\n",
            "Epoch 1, Batch 1600, Avg Loss over last 64 batches: 0.4624\n",
            "Epoch 1, Batch 1664, Avg Loss over last 64 batches: 0.4677\n",
            "Epoch 1, Batch 1728, Avg Loss over last 64 batches: 0.4354\n",
            "Epoch 1, Batch 1792, Avg Loss over last 64 batches: 0.4328\n",
            "Epoch 1, Batch 1856, Avg Loss over last 64 batches: 0.4280\n",
            "Epoch 1, Batch 1920, Avg Loss over last 64 batches: 0.4242\n",
            "Epoch 1, Batch 1984, Avg Loss over last 64 batches: 0.4200\n",
            "Epoch 1, Batch 2048, Avg Loss over last 64 batches: 0.4178\n",
            "Epoch 1, Batch 2112, Avg Loss over last 64 batches: 0.4236\n",
            "Epoch 1, Batch 2176, Avg Loss over last 64 batches: 0.4196\n",
            "Epoch 1, Batch 2240, Avg Loss over last 64 batches: 0.4082\n",
            "Epoch 1, Batch 2304, Avg Loss over last 64 batches: 0.4273\n",
            "Epoch 1, Batch 2368, Avg Loss over last 64 batches: 0.4413\n",
            "Epoch 1, Batch 2432, Avg Loss over last 64 batches: 0.4228\n",
            "Epoch 1, Batch 2496, Avg Loss over last 64 batches: 0.8738\n",
            "Epoch 1, Batch 2560, Avg Loss over last 64 batches: 0.7542\n",
            "Epoch 1, Batch 2624, Avg Loss over last 64 batches: 0.7271\n",
            "Epoch 1, Batch 2688, Avg Loss over last 64 batches: 0.7069\n",
            "Epoch 1, Batch 2752, Avg Loss over last 64 batches: 0.7022\n",
            "Epoch 1, Batch 2816, Avg Loss over last 64 batches: 0.7311\n",
            "Epoch 1, Batch 2880, Avg Loss over last 64 batches: 0.6954\n",
            "Epoch 1, Batch 2944, Avg Loss over last 64 batches: 0.6952\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-47-3759435773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mefficientnet_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-36-865446664.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, efficientnet_image, clip_image)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mefficientnet_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mefficientnet_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mclip_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclip_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mefficientnet_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mget_image_features\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    978\u001b[0m         )\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n\u001b[0m\u001b[1;32m    981\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_layrnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         encoder_outputs: BaseModelOutput = self.encoder(\n\u001b[0m\u001b[1;32m    786\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 )\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Validation Loop\n",
        "  model.eval() # Set model to evaluation mode\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  val_loss = 0.0\n",
        "  with torch.no_grad(): # Disable gradient calculation\n",
        "      for efficientnet_images, clip_images, labels in val_dataloader:\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(efficientnet_images, clip_images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          val_loss += loss.item() * efficientnet_images.size(0)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  epoch_val_loss = val_loss / len(val_dataset)\n",
        "  accuracy = 100 * correct / total\n",
        "  print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "jXkTzZ3sdd9o",
        "outputId": "81c001b3-d978-46a8-ab8d-1c486feac723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-48-3874325222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Disable gradient calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mefficientnet_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mefficientnet_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-39-270043891.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mefficientnet_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefficientNet_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mclip_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Move inputs to the device within the dataset item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-29-4021242226.py\u001b[0m in \u001b[0;36mclip_preprocess\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclip_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclip_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Move inputs to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclip_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclip_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/processing_clip.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mimage_processor_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchFeature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/image_processing_clip.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_resize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdo_center_crop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/clip/image_processing_clip.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0minput_data_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[0;32m--> 193\u001b[0;31m         return resize(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;31m# PIL images are in the format (width, height)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducing_gap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreducing_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_numpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2314\u001b[0m                 )\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_twjuoJpRe2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}